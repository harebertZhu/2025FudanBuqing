{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9863327",
   "metadata": {},
   "source": [
    "# <center>字符级Seq2Seq翻译模型实现</center>\n",
    "\n",
    "一个基于PyTorch的Seq2Seq代码展示了如何使用Python和PyTorch构建和训练一个序列到序列的机器翻译模型，进行英语到德语的文本翻译。。\n",
    "\n",
    "1. 设置环境与模型定义\n",
    "\n",
    "*  数据预处理：加载并清洗英德平行语料\n",
    "*  字符级分词：构建字符词汇表\n",
    "*  模型构建：实现编码器-解码器架构\n",
    "*  训练循环：使用Teacher Forcing策略\n",
    "*  评估与预测：实现翻译功能\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9785e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0853635",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8c1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English tokenizer size: 26\n",
      "German tokenizer size: 32\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# 字符级Seq2Seq翻译模型 - 字母表定义与Tokenizer初始化\n",
    "# 功能：定义英德字母表并创建对应的字符级Tokenizer\n",
    "# 说明：\n",
    "# 1. 英语字母表仅包含26个基本字母\n",
    "# 2. 德语字母表额外包含ä, ö, ü, ß等特殊字符\n",
    "# 3. 包含制表符\\t和换行符\\n作为特殊控制字符\n",
    "# ==============================================\n",
    "\n",
    "\n",
    "# 定义字母表\n",
    "english_alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "german_alphabet = 'abcdefghijklmnopqrstuvwxyzäöüß\\t\\n'\n",
    "\n",
    "# 创建英文的Tokenizer\n",
    "english_tokenizer = Tokenizer(char_level=True, lower=True)\n",
    "english_tokenizer.fit_on_texts([english_alphabet])\n",
    "english_word_index = english_tokenizer.word_index\n",
    "\n",
    "# 创建德文的Tokenizer\n",
    "german_tokenizer = Tokenizer(char_level=True, lower=True)\n",
    "german_tokenizer.fit_on_texts([german_alphabet])\n",
    "german_word_index = german_tokenizer.word_index\n",
    "\n",
    "# 验证字典大小（可以调整断言范围）\n",
    "print(f\"English tokenizer size: {len(english_word_index)}\")\n",
    "print(f\"German tokenizer size: {len(german_word_index)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2ee56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentence pairs: 278168\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# 数据加载与预处理模块\n",
    "# 功能：从deu-eng文件夹加载英德平行语料库\n",
    "# 处理流程：\n",
    "# 1. 遍历指定目录下的所有.txt文件\n",
    "# 2. 按制表符分割每行内容（格式：英文\\t德文\\t来源）\n",
    "# 3. 对文本进行标准化处理（转为小写）\n",
    "# 4. 为德文句子添加起始符\\t和终止符\\n\n",
    "# 注意事项：\n",
    "# - 文件编码必须为UTF-8以支持特殊字符\n",
    "# - 每行应有3个字段，否则会被过滤\n",
    "# - 最终输出英文和德文句子列表\n",
    "# ==============================================\n",
    "\n",
    "\n",
    "data_path = 'deu-eng'\n",
    "english_sentences = []\n",
    "german_sentences = []\n",
    "\n",
    "for file_name in os.listdir(data_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        with open(os.path.join(data_path, file_name), 'r', encoding='utf-8') as file:\n",
    "            for line in file.readlines():\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 3:\n",
    "                    english_sentences.append(parts[0].lower())\n",
    "                    german_sentences.append('\\t' + parts[1].lower() + '\\n')\n",
    "\n",
    "print(\"Total sentence pairs:\", len(english_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d18278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# 数据预处理与张量转换模块\n",
    "# 功能：将文本序列转换为模型可处理的张量格式\n",
    "# 处理流程：\n",
    "# 1. 使用Tokenizer将文本转换为数字序列\n",
    "# 2. 将数字序列转换为one-hot编码\n",
    "# 3. 计算编码器和解码器的最大序列长度\n",
    "# 4. 初始化三维零矩阵作为输入/输出容器\n",
    "# 5. 填充数据到固定长度矩阵中\n",
    "# 6. 将numpy数组转换为PyTorch张量\n",
    "# 注意事项：\n",
    "# - 英文和德文使用独立的Tokenizer\n",
    "# - one-hot编码维度为词汇表大小+1（保留0作为padding）\n",
    "# - 解码器目标数据比输入数据偏移一个时间步\n",
    "# - 最终输出三个张量：\n",
    "#   - encoder_input_data: (样本数, 最大英文长度, 英文词汇量)\n",
    "#   - decoder_input_data: (样本数, 最大德文长度, 德文词汇量) \n",
    "#   - decoder_target_data: 同上，作为训练目标\n",
    "# ==============================================\n",
    "\n",
    "\n",
    "\n",
    "english_sequences = english_tokenizer.texts_to_sequences(english_sentences)\n",
    "german_sequences = german_tokenizer.texts_to_sequences(german_sentences)\n",
    "\n",
    "english_onehot = [to_categorical(seq, num_classes=len(english_word_index) + 1) for seq in english_sequences]\n",
    "german_onehot = [to_categorical(seq, num_classes=len(german_word_index) + 1) for seq in german_sequences]\n",
    "\n",
    "max_encoder_seq_length = max(len(seq) for seq in english_sequences)\n",
    "max_decoder_seq_length = max(len(seq) for seq in german_sequences)\n",
    "\n",
    "encoder_input_data = np.zeros((len(english_sentences), max_encoder_seq_length, len(english_word_index) + 1), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(german_sentences), max_decoder_seq_length, len(german_word_index) + 1), dtype='float32')\n",
    "decoder_target_data = np.zeros_like(decoder_input_data)\n",
    "\n",
    "for i, (enc, dec) in enumerate(zip(english_onehot, german_onehot)):\n",
    "    encoder_input_data[i, :len(enc)] = enc\n",
    "    decoder_input_data[i, :len(dec)] = dec\n",
    "    decoder_target_data[i, :len(dec)-1] = dec[1:]\n",
    "\n",
    "# 转为 Tensor\n",
    "encoder_input_data = torch.tensor(encoder_input_data, dtype=torch.float32)\n",
    "decoder_input_data = torch.tensor(decoder_input_data, dtype=torch.float32)\n",
    "decoder_target_data = torch.tensor(decoder_target_data, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38c3ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder shape: torch.Size([64, 427, 27])\n",
      "Decoder input shape: torch.Size([64, 401, 33])\n",
      "Decoder target shape: torch.Size([64, 401, 33])\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# 自定义数据集类 TranslationDataset\n",
    "# 功能：封装Seq2Seq模型训练所需的数据加载逻辑\n",
    "# 继承自PyTorch的Dataset类，实现三个核心方法：\n",
    "# 1. __init__: 初始化编码器输入、解码器输入和目标输出\n",
    "# 2. __len__: 返回数据集样本总数\n",
    "# 3. __getitem__: 按索引返回单个样本\n",
    "# 使用说明：\n",
    "# - 配合DataLoader实现批量加载和随机打乱\n",
    "# - 输入数据应为预处理后的张量形式\n",
    "# - 输出三个张量分别对应：\n",
    "#   - 编码器输入 (英文序列)\n",
    "#   - 解码器输入 (德文序列)\n",
    "#   - 解码器目标 (偏移一个时间步的德文序列)\n",
    "# ==============================================\n",
    "\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, decoder_target):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.decoder_target = decoder_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.encoder_input.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoder_input[idx], self.decoder_input[idx], self.decoder_target[idx]\n",
    "\n",
    "dataset = TranslationDataset(encoder_input_data, decoder_input_data, decoder_target_data)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 验证数据形状\n",
    "batch = next(iter(dataloader))\n",
    "print(\"Encoder shape:\", batch[0].shape)\n",
    "print(\"Decoder input shape:\", batch[1].shape)\n",
    "print(\"Decoder target shape:\", batch[2].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05243c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqModel(\n",
      "  (encoder): LSTM(27, 256, batch_first=True)\n",
      "  (decoder): LSTM(33, 256, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=33, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# Seq2Seq 模型类定义\n",
    "# 功能：实现基于LSTM的编码器-解码器架构\n",
    "# 结构组成：\n",
    "# 1. 编码器LSTM：将输入序列编码为隐藏状态\n",
    "# 2. 解码器LSTM：基于编码器隐藏状态生成目标序列\n",
    "# 3. 全连接层：将解码器输出映射到目标词汇空间\n",
    "# 参数说明：\n",
    "# - input_dim: 输入维度（英文词汇量+1）\n",
    "# - output_dim: 输出维度（德文词汇量+1） \n",
    "# - hidden_dim: 隐藏层维度（默认256）\n",
    "# 前向传播流程：\n",
    "# 1. 编码器处理输入序列，输出最终隐藏状态\n",
    "# 2. 解码器使用编码器隐藏状态初始化\n",
    "# 3. 全连接层处理解码器输出\n",
    "# 注意事项：\n",
    "# - 使用batch_first=True保持数据维度一致\n",
    "# - 自动检测并使用GPU加速\n",
    "# ==============================================\n",
    "\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(output_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        _, (hidden, cell) = self.encoder(encoder_input)\n",
    "        decoder_output, _ = self.decoder(decoder_input, (hidden, cell))\n",
    "        return self.fc(decoder_output)\n",
    "\n",
    "input_dim = len(english_word_index) + 1\n",
    "output_dim = len(german_word_index) + 1\n",
    "hidden_dim = 256\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Seq2SeqModel(input_dim, output_dim, hidden_dim).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30b04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 4347/4347 [03:22<00:00, 21.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 4347/4347 [03:23<00:00, 21.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.1282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 4347/4347 [03:20<00:00, 21.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.1099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 4347/4347 [03:19<00:00, 21.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.1014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 4347/4347 [03:21<00:00, 21.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.0939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 4347/4347 [03:19<00:00, 21.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.1015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 4347/4347 [03:21<00:00, 21.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.1164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 4347/4347 [03:21<00:00, 21.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.1012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 4347/4347 [03:21<00:00, 21.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.1432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 4347/4347 [03:06<00:00, 23.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.1229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# 模型训练模块\n",
    "# 功能：执行Seq2Seq模型的训练过程\n",
    "# 训练流程：\n",
    "# 1. 定义损失函数(交叉熵损失)和优化器(Adam)\n",
    "# 2. 设置训练轮次(epoch)和初始学习率(0.001)\n",
    "# 3. 每个epoch中：\n",
    "#    - 将模型设为训练模式\n",
    "#    - 遍历数据加载器获取批次数据\n",
    "#    - 将数据移动到指定设备(CPU/GPU)\n",
    "#    - 清零梯度\n",
    "#    - 前向传播计算输出\n",
    "#    - 计算损失(输出与目标比较)\n",
    "#    - 反向传播计算梯度\n",
    "#    - 优化器更新参数\n",
    "#    - 累计epoch损失\n",
    "# 4. 打印每个epoch的平均损失\n",
    "# 5. 训练完成后保存模型参数\n",
    "# 注意事项：\n",
    "# - 使用tqdm显示训练进度条\n",
    "# - 损失计算时需reshape输出和目标张量\n",
    "# - 模型参数保存为seq2seq.pth文件\n",
    "# ==============================================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for encoder_input, decoder_input, decoder_target in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        encoder_input, decoder_input, decoder_target = encoder_input.to(device), decoder_input.to(device), decoder_target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(encoder_input, decoder_input)\n",
    "        loss = criterion(output.view(-1, output_dim), decoder_target.argmax(dim=2).view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "torch.save(model.state_dict(), \"seq2seq.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f827ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN: how are you\n",
      "DE: sichdiesenneurbestern\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# 句子翻译函数 translate_sentence\n",
    "# 功能：使用训练好的Seq2Seq模型进行单句翻译\n",
    "# 参数：\n",
    "# - model: 训练好的Seq2Seq模型\n",
    "# - sentence: 待翻译的英文句子\n",
    "# - max_length: 最大解码长度(默认100)\n",
    "# 处理流程：\n",
    "# 1. 将模型设为评估模式\n",
    "# 2. 对输入句子进行预处理和one-hot编码\n",
    "# 3. 初始化解码器输入(起始符\\t)\n",
    "# 4. 编码器处理输入获得隐藏状态\n",
    "# 5. 循环解码直到遇到终止符\\n或达到最大长度\n",
    "# 6. 每次解码步骤：\n",
    "#    - 使用当前隐藏状态预测下一个字符\n",
    "#    - 将预测字符作为下一时间步输入\n",
    "#    - 遇到终止符则停止解码\n",
    "# 返回：拼接后的德文翻译结果\n",
    "# 注意事项：\n",
    "# - 使用torch.no_grad()禁用梯度计算\n",
    "# - 自动处理设备转移(CPU/GPU)\n",
    "# ==============================================\n",
    "\n",
    "def translate_sentence(model, sentence, max_length=100):\n",
    "    model.eval()\n",
    "    seq = english_tokenizer.texts_to_sequences([sentence.lower()])\n",
    "    onehot = to_categorical(seq[0], num_classes=len(english_word_index) + 1)\n",
    "    input_tensor = torch.tensor(onehot, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    decoder_input = torch.zeros((1, 1, output_dim), dtype=torch.float32).to(device)\n",
    "    decoder_input[0, 0, german_word_index['\\t']] = 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, (hidden, cell) = model.encoder(input_tensor)\n",
    "\n",
    "    decoded = []\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            output, (hidden, cell) = model.decoder(decoder_input, (hidden, cell))\n",
    "            out = model.fc(output)\n",
    "            pred = torch.argmax(out, dim=2).item()\n",
    "            if pred == german_word_index['\\n']:\n",
    "                break\n",
    "            decoded.append(german_tokenizer.index_word.get(pred, ''))\n",
    "            decoder_input = torch.zeros((1, 1, output_dim), dtype=torch.float32).to(device)\n",
    "            decoder_input[0, 0, pred] = 1\n",
    "\n",
    "    return ''.join(decoded)\n",
    "\n",
    "# 示例翻译\n",
    "print(\"EN: how are you\")\n",
    "print(\"DE:\", translate_sentence(model, \"how are you\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
