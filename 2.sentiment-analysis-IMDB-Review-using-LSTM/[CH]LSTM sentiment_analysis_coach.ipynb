{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDB Reviews using LSTM and Keras(CH)\n",
    "created by Hans Michael\n",
    "<hr>\n",
    "\n",
    "### 步骤\n",
    "<ol type=\"1\">\n",
    "    <li>加载数据集（50K IMDB电影评论）</li>\n",
    "    <li>清洗数据集</li>\n",
    "    <li>编码情感</li>\n",
    "    <li>拆分数据集</li>\n",
    "    <li>对评论进行分词和填充/截断处理</li>\n",
    "    <li>构建模型架构</li>\n",
    "    <li>训练和测试</li>\n",
    "</ol>\n",
    "\n",
    "<hr>\n",
    "<i>导入所有库</i>\n",
    "\n",
    "\n",
    "NLTK（Natural Language Toolkit） 是一个用 Python 编写的自然语言处理（NLP）库，广泛用于文本处理、语言建模、语料库分析、词法分析、语法分析、情感分析等任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Harebert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd    # 导入 pandas 库，用于加载数据集\n",
    "import numpy as np     # 导入 numpy 库，用于数学运算\n",
    "from nltk.corpus import stopwords   # 导入 stopwords 来获取停用词集合\n",
    "from sklearn.model_selection import train_test_split       # 导入 train_test_split 函数，用于数据集划分\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # 导入 Tokenizer 类，用于将文本编码为整数\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # 导入 pad_sequences 函数，用于填充或截断序列\n",
    "from tensorflow.keras.models import Sequential     # 导入 Sequential 模型，用于建立模型\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # 导入模型层：Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # 导入 ModelCheckpoint 回调函数，用于保存模型\n",
    "from tensorflow.keras.models import load_model   # 导入 load_model 函数，用于加载保存的模型\n",
    "import re   # 导入 re 模块，用于正则表达式操作\n",
    "\n",
    "# 需要启动 NLTK Downloader 并下载所有所需的数据\n",
    "import nltk \n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<i>查看数据</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n",
      "['foo', 'bar', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('IMDB Dataset.csv')  # 加载IMDB数据集\n",
    "\n",
    "print(data)  # 打印数据集内容\n",
    "\n",
    "english_stops = set(stopwords.words('english'))  # 获取英文停用词集合\n",
    "sentence = \"this is a foo bar sentence\"  # 定义一个示例句子\n",
    "# 过滤掉停用词，并打印剩余单词列表\n",
    "print([i for i in sentence.split() if i not in english_stops])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b>停用词（Stop Word）</b> 是句子中常见的词语，通常搜索引擎会编程忽略这些词（例如 \"the\"、\"a\"、\"an\"、\"of\" 等）。\n",
    "\n",
    "<i>声明英文停用词</i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 加载和清洗数据集\n",
    "\n",
    "在原始数据集中，评论还未经过处理。仍然包含HTML标签、数字、大写字母和标点符号。这些内容不适合用于训练，因此在 <b>load_dataset()</b> 函数中，除了使用 <b>pandas</b> 加载数据集外，我还对评论进行预处理，包括去除HTML标签、非字母字符（标点符号和数字）、停用词，并将所有评论转换为小写。\n",
    "\n",
    "### 编码情感\n",
    "\n",
    "在同一个函数中，我还将情感编码为整数（0 和 1）。其中，0 表示负面情感，1 表示正面情感。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评论\n",
      "0        [one, reviewers, mentioned, watching, oz, epis...\n",
      "1        [a, wonderful, little, production, the, filmin...\n",
      "2        [i, thought, wonderful, way, spend, time, hot,...\n",
      "3        [basically, family, little, boy, jake, thinks,...\n",
      "4        [petter, mattei, love, time, money, visually, ...\n",
      "                               ...                        \n",
      "49995    [i, thought, movie, right, good, job, it, crea...\n",
      "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
      "49997    [i, catholic, taught, parochial, elementary, s...\n",
      "49998    [i, going, disagree, previous, comment, side, ...\n",
      "49999    [no, one, expects, star, trek, movies, high, a...\n",
      "Name: review, Length: 50000, dtype: object \n",
      "\n",
      "情感\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "49995    1\n",
      "49996    0\n",
      "49997    0\n",
      "49998    0\n",
      "49999    0\n",
      "Name: sentiment, Length: 50000, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harebert\\AppData\\Local\\Temp\\ipykernel_22560\\4115447060.py:16: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_data = y_data.replace('negative', 0)  # 将负面情感替换为0\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    df = pd.read_csv('IMDB Dataset.csv')  # 读取IMDB数据集\n",
    "    x_data = df['review']  # 评论/输入数据\n",
    "    y_data = df['sentiment']  # 情感/输出数据\n",
    "\n",
    "    # 预处理评论\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex=True)  # 移除HTML标签\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex=True)  # 移除非字母字符\n",
    "    # 移除停用词\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  \n",
    "    # 转换为小写\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])  \n",
    "\n",
    "    # 编码情感 -> 0 和 1\n",
    "    y_data = y_data.replace('positive', 1)  # 将正面情感替换为1\n",
    "    y_data = y_data.replace('negative', 0)  # 将负面情感替换为0\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "x_data, y_data = load_dataset()  # 加载并预处理数据集\n",
    "\n",
    "print('评论')\n",
    "print(x_data, '\\n')  # 打印评论数据\n",
    "print('情感')\n",
    "print(y_data)  # 打印情感数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 分割数据集\n",
    "\n",
    "在这项工作中，我决定使用 Scikit-Learn 中的 <b>train_test_split</b> 方法，将数据集分为80%的训练集和20%的测试集。使用这种方法时，它会自动对数据集进行洗牌。我们需要洗牌数据，因为在原始数据集中，评论和情感是有序排列的，先列出正面评论，然后是负面评论。通过洗牌数据，可以使数据在模型中分布均匀，从而对预测更准确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集\n",
      "39319    [touching, story, four, kids, run, away, foste...\n",
      "36125    [if, havn, seen, movie, i, highly, recommend, ...\n",
      "26426    [i, discovered, movie, accidentally, really, n...\n",
      "39203    [robert, deniro, plays, unbelievably, intellig...\n",
      "22217    [personally, i, problem, acting, script, i, pr...\n",
      "                               ...                        \n",
      "11413    [great, drama, areas, covered, except, screenl...\n",
      "456      [oh, goodness, i, would, never, thought, possi...\n",
      "39850    [excellent, performance, there, still, good, a...\n",
      "23677    [there, nothing, i, hate, self, congratulating...\n",
      "34557    [as, young, teenager, time, airwolf, compulsor...\n",
      "Name: review, Length: 40000, dtype: object \n",
      "\n",
      "39319    1\n",
      "36125    1\n",
      "26426    1\n",
      "39203    0\n",
      "22217    0\n",
      "        ..\n",
      "11413    1\n",
      "456      0\n",
      "39850    1\n",
      "23677    0\n",
      "34557    1\n",
      "Name: sentiment, Length: 40000, dtype: int64 \n",
      "\n",
      "测试集\n",
      "31583    [the, stepford, children, second, best, stepfo...\n",
      "40611    [superficically, brigadoon, promising, enterta...\n",
      "36816    [what, hell, movie, well, i, know, son, mask, ...\n",
      "12028    [the, headline, describes, exactly, this, drib...\n",
      "48526    [after, disappointing, part, i, kinda, wondere...\n",
      "                               ...                        \n",
      "41321    [although, time, revealed, effects, done, stor...\n",
      "23673    [crazy, scottish, warrior, race, stranded, dee...\n",
      "4009     [i, really, enjoyed, girl, fight, it, somethin...\n",
      "39099    [seeing, moonstruck, many, years, reminder, sw...\n",
      "21901    [first, i, like, start, saying, refreshing, st...\n",
      "Name: review, Length: 10000, dtype: object \n",
      "\n",
      "31583    1\n",
      "40611    0\n",
      "36816    0\n",
      "12028    0\n",
      "48526    1\n",
      "        ..\n",
      "41321    1\n",
      "23673    1\n",
      "4009     1\n",
      "39099    1\n",
      "21901    1\n",
      "Name: sentiment, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2)  # 将数据集按80%训练集和20%测试集进行划分\n",
    "\n",
    "print('训练集')\n",
    "print(x_train, '\\n')  # 打印训练集输入数据\n",
    "print(y_train, '\\n')  # 打印训练集输出数据\n",
    "print('测试集')\n",
    "print(x_test, '\\n')  # 打印测试集输入数据\n",
    "print(y_test)  # 打印测试集输出数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<i>通过计算所有评论长度的平均值（使用 <b>numpy.mean</b> 函数）来获取最大评论长度的函数</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    review_length = []  # 初始化一个列表来存储每条评论的长度\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))  # 将每条评论的长度添加到列表中\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))  # 返回所有评论长度的平均值，并向上取整\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 分词并填充/截断评论\n",
    "\n",
    "神经网络只接受数值数据，因此我们需要对评论进行编码。我使用 <b>tensorflow.keras.preprocessing.text.Tokenizer</b> 将评论编码为整数，其中每个唯一单词根据 <b>x_train</b> 自动索引（使用 <b>fit_on_texts</b> 方法）。<br>\n",
    "<b>x_train</b> 和 <b>x_test</b> 被转换为整数，使用 <b>texts_to_sequences</b> 方法。\n",
    "\n",
    "每条评论的长度不同，因此我们需要使用 <b>tensorflow.keras.preprocessing.sequence.pad_sequences</b> 在填充（添加0）或截断单词到相同长度（在这种情况下，是所有评论长度的平均值）。\n",
    "\n",
    "<b>post</b>，在句子的末尾填充或截断单词<br>\n",
    "<b>pre</b>，在句子的开头填充或截断单词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码后的训练集\n",
      " [[ 1268    14   568 ...     0     0     0]\n",
      " [   54 57015    38 ...     0     0     0]\n",
      " [    1  1860     3 ...     1  1809   202]\n",
      " ...\n",
      " [  224   150    49 ...     0     0     0]\n",
      " [   49    76     1 ...     0     0     0]\n",
      " [  108    96  2108 ... 10766   957   198]] \n",
      "\n",
      "编码后的测试集\n",
      " [[    2 16930   316 ...     0     0     0]\n",
      " [28557  2331   608 ...  2562  1328   834]\n",
      " [  107   478     3 ...     0     0     0]\n",
      " ...\n",
      " [    1    13   425 ...     0     0     0]\n",
      " [  223 12181    37 ... 19235  8717   213]\n",
      " [   23     1     6 ...   917   720   634]] \n",
      "\n",
      "最大评论长度:  130\n"
     ]
    }
   ],
   "source": [
    "# 编码评论\n",
    "token = Tokenizer(lower=False)  # 不需要转换为小写，因为在 load_data() 中已经转换\n",
    "token.fit_on_texts(x_train)  # 使用训练集的文本进行分词器拟合\n",
    "x_train = token.texts_to_sequences(x_train)  # 将训练集文本转换为整数序列\n",
    "x_test = token.texts_to_sequences(x_test)  # 将测试集文本转换为整数序列\n",
    "\n",
    "max_length = get_max_length()  # 获取评论的最大长度\n",
    "\n",
    "# 对训练集和测试集进行填充和截断\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1  # 计算总词汇数，加1是因为有0填充\n",
    "\n",
    "print('编码后的训练集\\n', x_train, '\\n')  # 打印编码后的训练集\n",
    "print('编码后的测试集\\n', x_test, '\\n')  # 打印编码后的测试集\n",
    "print('最大评论长度: ', max_length)  # 打印最大评论长度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 构建模型架构\n",
    "\n",
    "<b>嵌入层（Embedding Layer）</b>：简单来说，它为 <i>word_index</i> 中的每个单词创建词向量，并通过分析周围的其他单词来分组相关或具有类似含义的单词。\n",
    "\n",
    "<b>LSTM 层</b>：通过考虑当前输入、先前输出和先前记忆来决定保留还是丢弃数据。LSTM 中有一些重要的组成部分：\n",
    "<ul>\n",
    "    <li><b>遗忘门（Forget Gate）</b>：决定是否保留信息或丢弃信息</li>\n",
    "    <li><b>输入门（Input Gate）</b>：通过将先前的输出和当前输入传递到 sigmoid 激活函数来更新细胞状态</li>\n",
    "    <li><b>细胞状态（Cell State）</b>：计算新的细胞状态，它与遗忘向量相乘（如果乘以接近 0 的值则丢弃），加上来自输入门的输出以更新细胞状态值。</li>\n",
    "    <li><b>输出门（Output Gate）</b>：决定下一个隐藏状态，并用于预测</li>\n",
    "</ul>\n",
    "\n",
    "<b>全连接层（Dense Layer）</b>：使用权重矩阵和偏置（可选）计算输入，并使用激活函数。我在这项工作中使用 <b>Sigmoid</b> 激活函数，因为输出只有 0 或 1。\n",
    "\n",
    "优化器为 <b>Adam</b>，损失函数为 <b>Binary Crossentropy</b>，因为输出只有 0 和 1，是一个二进制数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBED_DIM 32\n",
      "LSTM_OUT 64\n",
      "max_length 130\n"
     ]
    }
   ],
   "source": [
    "# ARCHITECTURE\n",
    "EMBED_DIM = 32\n",
    "LSTM_OUT = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "print(\"EMBED_DIM\",EMBED_DIM)\n",
    "print(\"LSTM_OUT\",LSTM_OUT)\n",
    "print(\"max_length\",max_length)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 训练\n",
    "\n",
    "训练过程很简单。我们只需要拟合我们的 <b>x_train</b>（输入）和 <b>y_train</b>（输出/标签）数据。在这个训练中，我使用了小批量学习方法，<b>批量大小（batch_size）</b> 设置为 <i>128</i>，<b>epochs</b> 设置为 <i>5</i>。\n",
    "\n",
    "此外，我添加了一个名为 **checkpoint** 的回调函数，如果模型的准确率比上一个 epoch 改善，则在每个 epoch 结束时将模型保存在本地。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'models/LSTM.keras',  # 模型保存路径\n",
    "    monitor='accuracy',  # 监控的指标为准确率\n",
    "    save_best_only=True,  # 仅保存最优模型\n",
    "    verbose=1  # 启用详细输出\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m312/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5710 - loss: 0.6608\n",
      "Epoch 1: accuracy improved from -inf to 0.62608, saving model to models/LSTM.keras\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 44ms/step - accuracy: 0.5714 - loss: 0.6606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c6a46ddf10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train,  # 训练集输入数据\n",
    "    y_train,  # 训练集输出数据\n",
    "    batch_size=128,  # 批量大小\n",
    "    epochs=1,  # 训练轮数\n",
    "    callbacks=[checkpoint]  # 回调函数列表，包括检查点\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 打印查看模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_18\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_18\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,955,904</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_18 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m2,955,904\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_18 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m24,832\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,942,405</span> (34.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,942,405\u001b[0m (34.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,980,801</span> (11.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,980,801\u001b[0m (11.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,961,604</span> (22.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m5,961,604\u001b[0m (22.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 测试\n",
    "\n",
    "为了评估模型，我们需要使用我们的 <b>x_test</b> 数据进行情感预测，并将预测结果与 <b>y_test</b>（期望输出）数据进行比较。然后，通过将正确预测的数量除以总数据量来计算模型的准确率。结果显示准确率为 <b>?%</b>。(根据每次训练的参数，该数字会有不同)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step\n",
      "正确预测数: 6007\n",
      "错误预测数: 3993\n",
      "准确率: 60.07%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = model.predict(x_test, batch_size=128)  # 对测试集进行预测\n",
    "\n",
    "# 将预测值转换为二进制（0或1），阈值为0.5\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "true = 0\n",
    "for i, y in enumerate(y_test):\n",
    "    if y == y_pred[i]:\n",
    "        true += 1  # 计算正确预测的数量\n",
    "\n",
    "print('正确预测数: {}'.format(true))  # 打印正确预测的数量\n",
    "print('错误预测数: {}'.format(len(y_pred) - true))  # 打印错误预测的数量\n",
    "print('准确率: {:.2f}%'.format(true / len(y_pred) * 100))  # 打印准确率，保留两位小数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 加载已保存的模型\n",
    "\n",
    "加载已保存的模型，并使用它来预测电影评论的情感（正面或负面）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('models/LSTM.keras') # 加载已保存的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接收一个评论作为输入进行预测\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = str(input('Movie Review: '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入必须在传递给模型进行预测之前进行预处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清理后的评论:  It is really bad\n",
      "过滤后的评论:  ['it really bad']\n"
     ]
    }
   ],
   "source": [
    "# 预处理输入\n",
    "regex = re.compile(r'[^a-zA-Z\\s]')  # 定义正则表达式，用于匹配非字母字符和空格\n",
    "review = regex.sub('', review)  # 使用正则表达式移除非字母字符\n",
    "print('清理后的评论: ', review)  # 打印清理后的评论\n",
    "\n",
    "words = review.split(' ')  # 将评论拆分成单词列表\n",
    "# 过滤掉停用词\n",
    "filtered = [w for w in words if w not in english_stops]\n",
    "filtered = ' '.join(filtered)  # 将过滤后的单词列表重新组合成字符串\n",
    "filtered = [filtered.lower()]  # 转换为小写，并放入列表中\n",
    "\n",
    "print('过滤后的评论: ', filtered)  # 打印过滤后的评论\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要再次对单词进行分词和编码。我使用之前声明的分词器，因为我们想根据模型已知的单词对其进行编码。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7 13 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# 将过滤后的评论分词并转换为序列\n",
    "tokenize_words = token.texts_to_sequences(filtered)\n",
    "# 对序列进行填充和截断，使其长度与训练时的一致\n",
    "tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
    "print(tokenize_words)  # 打印分词后的序列\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是预测结果，显示了评论的 **置信度分数**。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "[[0.41574645]]\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model.predict(tokenize_words)  # 使用加载的模型进行预测\n",
    "print(result)  # 打印预测结果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果置信度分数接近0，则该评论为 **负面**。另一方面，如果置信度分数接近1，则该评论为 **正面**。我使用 **0.7** 作为判断正面和负面置信度分数的阈值，因此如果置信度分数等于或大于0.7，则为 **正面**；如果小于0.7，则为 **负面**。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "if result >= 0.7:\n",
    "    print('positive')  # 如果预测结果大于等于0.7，打印正面情感\n",
    "else:\n",
    "    print('negative')  # 否则，打印负面情感\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
